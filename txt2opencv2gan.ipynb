{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip isntall -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'config' from 'c:\\\\Users\\\\kuzga\\\\OneDrive\\\\Рабочий стол\\\\ShiftLab-2024-Computer-Vision\\\\config.py'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "\n",
    "import importlib\n",
    "import config\n",
    "importlib.reload(config)\n",
    "# from datasets import load_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try:\n",
    "#     dataset = load_dataset('DonkeySmall/OCR-English-Printed-12', split='train[:1%]')\n",
    "#     print(\"Данные загружены успешно!\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Ошибка при загрузке набора данных: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv2_show(img, key='q', time=0, window_name='cv2'):\n",
    "    print(time)\n",
    "    cv2.imshow(window_name, img)\n",
    "    if time:\n",
    "        cv2.waitKey(time)\n",
    "        cv2.destroyAllWindows()\n",
    "    else:\n",
    "        printed_key = cv2.waitKey(0)\n",
    "        if printed_key == key:\n",
    "            cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "def stack_images(images, direction='vertical'):\n",
    "    if not images or any(img is None for img in images):\n",
    "        raise ValueError(\n",
    "            \"Список изображений пуст или содержит недопустимые значения\")\n",
    "\n",
    "    if direction == 'horizontal':\n",
    "        min_height = min(img.shape[0] for img in images)\n",
    "        resized_images = [cv2.resize(img, (int(img.shape[1] * (min_height / img.shape[0])), min_height))\n",
    "                          for img in images]\n",
    "        stacked_image = np.hstack(resized_images)\n",
    "    elif direction == 'vertical':\n",
    "        min_width = min(img.shape[1] for img in images)\n",
    "        resized_images = [cv2.resize(img, (min_width, int(img.shape[0] * (min_width / img.shape[1]))))\n",
    "                          for img in images]\n",
    "        stacked_image = np.vstack(resized_images)\n",
    "    else:\n",
    "        raise ValueError(\"direction должен быть 'horizontal' или 'vertical'\")\n",
    "\n",
    "    return stacked_image\n",
    "\n",
    "\n",
    "def add_noise_and_distortion(img):\n",
    "    noise = np.random.uniform(0, 50, img.shape).astype(np.uint8)\n",
    "    noisy_img = cv2.add(img, noise)\n",
    "\n",
    "    # искажения с помощью линий\n",
    "    num_lines = np.random.randint(0, 10)\n",
    "    for _ in range(num_lines):\n",
    "        x1, y1 = np.random.randint(\n",
    "            0, img.shape[1]), np.random.randint(0, img.shape[0])\n",
    "        x2, y2 = np.random.randint(\n",
    "            0, img.shape[1]), np.random.randint(0, img.shape[0])\n",
    "        cv2.line(noisy_img, (x1, y1), (x2, y2), (0, 0, 0), 1)\n",
    "\n",
    "    return noisy_img\n",
    "\n",
    "\n",
    "def generate_text_image(text, img=None, noise=False):\n",
    "    gen_height, gen_width = 64, 256\n",
    "\n",
    "    if img is None:\n",
    "        background_color = np.random.randint(0, 256, size=3, dtype=np.uint8)\n",
    "        img = np.ones((gen_height, gen_width, 3), dtype=np.uint8) * background_color\n",
    "\n",
    "    # Выбираем случайный шрифт\n",
    "    font_path = random.choice(config.custom_fonts_paths)\n",
    "    font_size = random.randint(20, 35)\n",
    "    font = ImageFont.truetype(font_path, font_size)\n",
    "\n",
    "    pil_img = Image.fromarray(img)\n",
    "    draw = ImageDraw.Draw(pil_img)\n",
    "\n",
    "    # Вычисляем размер текста и его позицию\n",
    "    bbox = draw.textbbox((0, 0), text, font=font)\n",
    "    text_size = (bbox[2] - bbox[0], bbox[3] - bbox[1])\n",
    "    while text_size[0] > img.shape[1] or text_size[1] > img.shape[0]:\n",
    "        font_size -= 1\n",
    "        font = ImageFont.truetype(font_path, font_size)\n",
    "        bbox = draw.textbbox((0, 0), text, font=font)\n",
    "        text_size = (bbox[2] - bbox[0], bbox[3] - bbox[1])\n",
    "\n",
    "    offset = random.randint(-5, 5)  # Значение смещения от -5 до 5 пикселей\n",
    "    text_x = (img.shape[1] - text_size[0]) // 2 + offset  # Центрируем текст и добавляем смещение\n",
    "    text_y = (img.shape[0] - text_size[1]) // 2\n",
    "\n",
    "    # Цвет текста\n",
    "    background_color = np.mean(img, axis=(0, 1)).astype(int).tolist()\n",
    "    text_color = tuple([255 - c for c in background_color])\n",
    "\n",
    "    for char in text:\n",
    "        # Получаем размер текущей буквы\n",
    "        bbox = draw.textbbox((text_x, text_y), char, font=font)\n",
    "        char_width = bbox[2] - bbox[0]\n",
    "        char_height = bbox[3] - bbox[1]\n",
    "        x_offset = random.randint(round(char_width * (-0.01)), round(char_width * (0.01)) // 1)\n",
    "        y_offset = random.randint(round(char_height * (-0.1)) // 1, round(char_height * (0.1)) // 1)\n",
    "        draw.text((text_x + x_offset, text_y + y_offset), char, font=font, fill=text_color)\n",
    "\n",
    "        # Увеличиваем x для следующей буквы\n",
    "        text_x += char_width  # Сдвигаем на ширину текущей буквы\n",
    "\n",
    "    # Преобразуем обратно в формат OpenCV\n",
    "    img = np.array(pil_img)\n",
    "\n",
    "    # Добавляем шум и искажения, если требуется\n",
    "    if noise:\n",
    "        return add_noise_and_distortion(img)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "images = []\n",
    "\n",
    "for _ in range(10):\n",
    "    text = random.choice(config.texts)\n",
    "    image = generate_text_image(text, noise=False)\n",
    "    images.append(image)\n",
    "\n",
    "stacked_images = stack_images(images)\n",
    "cv2_show(stacked_images, time=10000)\n",
    "# cv2_show(stacked_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 256)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "\n",
    "class TextImageDataset(Dataset):\n",
    "    def __init__(self, texts, transform=None):\n",
    "        self.texts = texts\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        img = generate_text_image(text)\n",
    "        # img = add_noise_and_distortion(img)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)  # Переводим в ч/б\n",
    "        img = img / 255.0  # Нормализуем\n",
    "\n",
    "        img = Image.fromarray((img * 255).astype('uint8'))\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, text\n",
    "\n",
    "\n",
    "dataset = TextImageDataset(config.texts, transform=transform)\n",
    "img, _ = dataset[0]\n",
    "plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, nz, ngf, nc):\n",
    "        super(Generator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.ConvTranspose2d(nz, ngf * 8, (4, 16), 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(ngf * 2, nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, nc, ndf):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(nc, ndf, 4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(ndf * 8 * 4 * 16, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (16x2048 and 8192x1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 49\u001b[0m\n\u001b[0;32m     46\u001b[0m fake_images \u001b[38;5;241m=\u001b[39m generator(noise)\n\u001b[0;32m     47\u001b[0m labels\u001b[38;5;241m.\u001b[39mfill_(\u001b[38;5;241m0.0\u001b[39m)\n\u001b[1;32m---> 49\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mdiscriminator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfake_images\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     50\u001b[0m loss_fake \u001b[38;5;241m=\u001b[39m criterion(output, labels)\n\u001b[0;32m     51\u001b[0m loss_fake\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\kuzga\\OneDrive\\Рабочий стол\\ShiftLab-2024-Computer-Vision\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kuzga\\OneDrive\\Рабочий стол\\ShiftLab-2024-Computer-Vision\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[12], line 46\u001b[0m, in \u001b[0;36mDiscriminator.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     44\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(x)\n\u001b[0;32m     45\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 46\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\kuzga\\OneDrive\\Рабочий стол\\ShiftLab-2024-Computer-Vision\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kuzga\\OneDrive\\Рабочий стол\\ShiftLab-2024-Computer-Vision\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kuzga\\OneDrive\\Рабочий стол\\ShiftLab-2024-Computer-Vision\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (16x2048 and 8192x1)"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.utils as vutils\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "num_epochs = 50\n",
    "lr = 0.0002\n",
    "beta1 = 0.5\n",
    "\n",
    "batch_size = 16\n",
    "nz = 100\n",
    "ngf = 16\n",
    "ndf = 16\n",
    "nc = 1\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = 'cpu'\n",
    "\n",
    "generator = Generator(nz, ngf, nc).to(device)\n",
    "discriminator = Discriminator(nc, ndf).to(device)\n",
    "\n",
    "optimizerD = optim.Adam(discriminator.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "optimizerG = optim.Adam(generator.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "fixed_noise = torch.randn(batch_size, nz, 1, 1, device=device)\n",
    "\n",
    "D_losses = []\n",
    "G_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        discriminator.zero_grad()\n",
    "        real_images = data[0].to(device)\n",
    "        batch_size = real_images.size(0)\n",
    "        labels = torch.full((batch_size,), 1.0, dtype=torch.float, device=device)\n",
    "\n",
    "        output = discriminator(real_images).view(-1)\n",
    "        loss_real = criterion(output, labels)\n",
    "        loss_real.backward()\n",
    "\n",
    "        noise = torch.randn(batch_size, nz, 1, 1, device=device)\n",
    "        fake_images = generator(noise)\n",
    "        labels.fill_(0.0)\n",
    "\n",
    "        output = discriminator(fake_images.detach()).view(-1)\n",
    "        loss_fake = criterion(output, labels)\n",
    "        loss_fake.backward()\n",
    "\n",
    "        D_loss = loss_real + loss_fake\n",
    "        optimizerD.step()\n",
    "\n",
    "        generator.zero_grad()\n",
    "        labels.fill_(1.0)\n",
    "        output = discriminator(fake_images).view(-1)\n",
    "        G_loss = criterion(output, labels)\n",
    "        G_loss.backward()\n",
    "        optimizerG.step()\n",
    "\n",
    "        D_losses.append(D_loss.item())\n",
    "        G_losses.append(G_loss.item())\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}] | Step [{i}/{len(dataloader)}] | D_loss: {D_loss.item():.4f} | G_loss: {G_loss.item():.4f}\")\n",
    "\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        with torch.no_grad():\n",
    "            fake_images = generator(fixed_noise).detach().cpu()\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.imshow(np.transpose(vutils.make_grid(fake_images, padding=2, normalize=True), (1, 2, 0)))\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "torch.save(generator.state_dict(), 'generator.pth')\n",
    "torch.save(discriminator.state_dict(), 'discriminator.pth')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
